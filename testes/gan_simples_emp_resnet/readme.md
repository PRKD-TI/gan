# ğŸ§ª Primeiro Teste - Esqueleto PadrÃ£o da GAN

Este repositÃ³rio contÃ©m a estrutura mÃ­nima de uma **GAN para costura de imagens**, pensada como esqueleto de testes iniciais. A ideia Ã© ter uma base simples, limpa e fÃ¡cil de expandir.

---

## ğŸ“‚ Estrutura de DiretÃ³rios

```
.
â”œâ”€â”€ generator/
â”‚   â””â”€â”€ gen_base.py          # Gerador base (Encoder-Decoder simples)
â”œâ”€â”€ discriminator/
â”‚   â””â”€â”€ disc_base.py         # Discriminador base (PatchGAN simples)
â”œâ”€â”€ testes/
â”‚   â””â”€â”€ _esqueleto_padrao/
â”‚       â”œâ”€â”€ train.py         # Script principal de treino
â”‚       â””â”€â”€ train_loop.py    # Loop de treinamento
â””â”€â”€ utils/
    â””â”€â”€ dataset_utils.py     # Dataset customizado para costura de imagens
```

---

## ğŸ¨ Gerador - `generator/gen_base.py`

O gerador inicial (`BasicGenerator`) Ã© baseado em uma **arquitetura Encoder-Decoder** que recebe duas imagens (`part1`, `part2`) e concatena internamente.

**Arquitetura:**

- **Entrada:** `part1` (RGB), `part2` (RGB) â†’ concatenaÃ§Ã£o `[B, 6, H, W]`
- **Encoder:** 3 camadas convolucionais com downsampling progressivo (6 â†’ 64 â†’ 128 â†’ 256)
- **Bottleneck:** camada convolucional profunda (256 â†’ 512) para capturar contexto global
- **Decoder:** 3 camadas `ConvTranspose2d` para upsampling (512 â†’ 256 â†’ 128 â†’ 64)
- **SaÃ­da:** imagem RGB `[B, 3, H, W]` com ativaÃ§Ã£o `Tanh` entre -1 e 1

**ObservaÃ§Ã£o:** Este gerador serve como baseline experimental, fÃ¡cil de substituir por arquiteturas mais complexas no futuro.

---

## ğŸ•µï¸ Discriminador - `discriminator/disc_base.py`

O discriminador inicial (`BasicDiscriminator`) segue a ideia de um **PatchGAN condicional**.

**Arquitetura:**

- **Entrada:** concatenaÃ§Ã£o de `part1`, `part2` e imagem real/fake â†’ `[B, 9, H, W]`
- 4 camadas convolucionais progressivamente mais profundas (64 â†’ 512 filtros)
- BatchNorm aplicado a partir da 2Âª camada
- AtivaÃ§Ã£o **LeakyReLU(0.2)** em todas as camadas
- Camada final `Conv2d(512 â†’ 1)` produz mapa de confianÃ§a para cada patch da imagem

**SaÃ­da:** mapa de patches `[B, 1, H/16, W/16]` representando real/falso em cada regiÃ£o da imagem.

---

## ğŸ” Loop de Treinamento - `train_loop.py`

O loop implementa a lÃ³gica completa do treino da GAN:

1. **Learning Rates Fixos**

   - Adam com `lr_g=0.0002` e `lr_d=0.0002`
   - Mantidos constantes durante todas as Ã©pocas

2. **Forward do Gerador**

   - Recebe `part1` e `part2`, concatena internamente
   - Gera imagem fake para o discriminador

3. **Treino do Discriminador**

   - Entrada real: concatenaÃ§Ã£o `[part1, part2, target]`
   - Entrada fake: concatenaÃ§Ã£o `[part1, part2, fake.detach()]`
   - Loss: mÃ©dia de `loss_real` e `loss_fake` usando `BCEWithLogitsLoss`
   - Gradient clipping aplicado para estabilidade

4. **Treino do Gerador**

   - Objetivo: enganar o discriminador (`loss_GAN`) + proximidade com ground truth (`L1 Loss`) + similaridade perceptual (`VGG Loss`)
   - **VGG Perceptual Loss:** compara ativaÃ§Ã£o de camadas intermediÃ¡rias do VGG16
   - **LPIPS:** mÃ©trica perceptual adicional usada para monitoramento
   - Loss total: `loss_G = 8.0 * loss_GAN + 2.0 * L1 + vgg_weight * loss_VGG`

5. **Amostras Fixas**

   - Salva imagens geradas de um subset fixo a cada `fixeSampleTime` minutos

6. **Checkpoints**

   - Salvamento por batch e por Ã©poca, permitindo reinÃ­cio do treinamento

7. **TensorBoard**

   - Monitoramento de losses (`GAN`, `L1`, `VGG`, `Generator`, `Discriminator`) e mÃ©tricas perceptuais (`LPIPS`, PSNR, SSIM, MS-SSIM, L1)

**ObservaÃ§Ãµes:**

- GenSteps atualmente fixos (`max_gen_steps`) para simplificar o primeiro teste.
- Gradientes do gerador e discriminador sÃ£o clipados para `max_norm=1.0`.
- O loop estÃ¡ preparado para futuramente implementar **gen\_steps adaptativo** ou **learning rate scheduler**.

---

## ğŸ“Š MÃ©tricas Monitoradas

- `Loss_G`: perda total do gerador (GAN + L1 + VGG)
- `Loss_D`: perda do discriminador (real + fake)
- `Loss_VGG`: componente perceptual do VGG
- `LPIPS`: mÃ©trica perceptual
- `PSNR`, `SSIM`, `MS-SSIM`, `L1`
- Monitoramento de uso de CPU, RAM e GPU

---

## ğŸš€ PrÃ³ximos Passos

1. Validar o treinamento com dataset reduzido
2. Visualizar algumas amostras geradas
3. Explorar learning rates dinÃ¢micos e gen\_steps adaptativos
4. Evoluir para arquiteturas mais profundas (UNet, DualEncoder, atenÃ§Ã£o, etc.)
5. Incluir mÃ©tricas adicionais para avaliaÃ§Ã£o quantitativa

